{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf4e548-74c8-4f1e-8739-0020f4dd3d37",
   "metadata": {},
   "source": [
    "# Data Cleaning and Initial EDA Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f69e57-5e30-4b77-8261-5403eebe2628",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540e798-4580-4853-b97d-5f8c62a20d08",
   "metadata": {},
   "source": [
    "The goal of this project is to gather data from two different subreddits ([*r/pregnant*](https://www.reddit.com/r/Parenting/) & [*r/pareting*](https://www.reddit.com/r/pregnant/)) and then use Natural Language Processing (NLP) to train a classifier to be able to determine which subreddit a given post comes from. I chose r/parenting and r/pregnant as comparisons for this project as my partner and I are currently expecting our first child and I was interested in looking at the differences in language for individuals who are seeking support and/or posting about their pregnancy and those who are doing the same as parents. To examine the differences I will be conducting two classification models using Random Random Forests and logistic regression. A successful project will be one where the classification model is able to accurately predict which subreddit a post comes from based on its text. I believe the results of this project may give first-time parents an idea of the changes that occur when transitioning from pregnanancy to becoming a parent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42131694-5339-4b65-ade5-6469f363ed65",
   "metadata": {},
   "source": [
    "## Library Imports and Reading in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a502679-e330-493f-8432-fc08a4722dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#NLTK or Natural Language Toolkit imports (based on imports from lesson 5.03)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "pd.set_option('display.max_colwidth' ,999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beccba75-d87d-4239-99b5-d2ecc3442836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial CSVs from scrape uncomment to see the initial raw data or check in data folder\n",
    "#preg_df = pd.read_csv('./data/Raw_data/pregnant_submissions.csv').drop(columns= 'Unnamed: 0')\n",
    "#parent_df = pd.read_csv('./data/Raw_data/parent_submissions.csv').drop(columns= 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63e4811-e84e-450a-91c5-10ae53ce09c4",
   "metadata": {},
   "source": [
    "## Data Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a33dea-7df9-4b7a-a269-e2f05c850402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparison of unique posts from each reddit\n",
    "#based on the raw data using dataframes in the cell above\n",
    "#print(f'Out of', preg_df[['selftext']].shape[0], 'rows from r/pregnant,', preg_df[['selftext']].nunique()[0], 'are unique!')\n",
    "#print(f'Out of', parent_df[['selftext']].shape[0], 'rows from r/parenting,', parent_df[['selftext']].nunique()[0], 'are unique!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638a0fe6-0fa5-4999-b3c7-2a9bc38753b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSVs with lems after tokenizing and lemmatizing\n",
    "preg_df = pd.read_csv('./data/Lem_data/pregnant_lems.csv').drop(columns= 'Unnamed: 0')\n",
    "parent_df = pd.read_csv('./data/Lem_data/parent_lems.csv').drop(columns= 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fec3c7-db9d-42bc-b639-731d2fc2dffe",
   "metadata": {},
   "source": [
    "#### Investigations Author with highest number of posts for r/pregnant and r/parenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5c07e75-8520-4a37-96cd-12cd90bf1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment individual lines to run each\n",
    "#investigating Authors on r/pregnant\n",
    "#preg_df['author'].nunique() # = 35_417\n",
    "#top_10posters = preg_df['author'].value_counts()[:11]\n",
    "#preg_df['author'].value_counts()[0:101] #top 100 authors have made between 34 and 100 posts on r/pregnant\n",
    "#preg_df['author'].value_counts()[:1500] #top 1500 postes have made 10 or more posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bc0763a-31fe-4dd0-a66f-dc6af5214b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment individual lines to run each\n",
    "#parent_df['author'].nunique() #= 51150\n",
    "#parent_df['author'].value_counts()[0:11] #top ten authors made 50 -76 posts\n",
    "#parent_df['author'].value_counts()[:51] #top 50 authors made 23 - 76 posts \n",
    "#parent_df['author'].value_counts()[:101] #top 100 authors made 17 - 76 posts \n",
    "#parent_df['author'].value_counts()[0:386] # top 385 have made over 10 posts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a5d70b-df78-4fa1-83a8-50e9cf637c43",
   "metadata": {},
   "source": [
    "### Brief description of the data scraped from each subreddit:\n",
    "* [r/pregnant](https://www.reddit.com/r/pregnant/) :\n",
    "    * Time Frame from Saturday, May 27, 2017 7:44:08 AM MST to Saturday, October 30, 2021 6:53:17 PM MST\n",
    "    * Out of 99,978 posts scraped, 94,671 were unique posts.\n",
    "    * 35,419 unique authors, meaning that on average each author has made 2.67 different posts in this time frame.\n",
    "        * However the actual spread of posts is fairly top heavy with the top 1500 authors making 10 or more posts\n",
    "\n",
    "* [r/parenting](https://www.reddit.com/r/parenting/) :\n",
    "    * Time Frame from Sunday, February 10, 2019 2:57:49 PM MST to Saturday, October 30, 2021 6:45:37 PM MST\n",
    "    * Out of 99,988 posts scraped, 79,174 were unique posts.\n",
    "    * 51,150 unique authors, meaning that on average each author has made 1.55 posts in this time frame\n",
    "        * However the actual spread of posts is largely top heavy with the top 385 authors making 10 or more posts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8191ab11-b9a2-45d7-ab1d-bcd5d72d0e16",
   "metadata": {},
   "source": [
    "## EDA/Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241aa0d4-f399-4e68-9e1e-d02240edb153",
   "metadata": {},
   "source": [
    "### 1. Duplicates and Rearranging Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9300bc1b-8c46-4bfe-b115-bfef9c6d3129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.duplicated.html & \n",
    "#https://pandas.pydata.org/docs/reference/api/pandas.Series.drop_duplicates.html#pandas.Series.drop_duplicates\n",
    "# 5306 duplicates for r/pregnant\n",
    "preg_df.drop_duplicates(subset= ['selftext'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf64a7cd-c28e-4040-b321-32132bca76ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#20_814 dupliates in r/parenting\n",
    "parent_df.drop_duplicates(subset= ['selftext'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b7e6f48-f1dc-4941-a6bb-0b9b6abb9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no longer need the created_utc column as that was solely to ensure that the web scrape loop kept pulling new data\n",
    "#was used to drop from the raw data\n",
    "#preg_df.drop(columns=['created_utc'], inplace= True)\n",
    "#parent_df.drop(columns=['created_utc'], inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8c60fb-9bb1-4dd7-8c2d-bc875aa99337",
   "metadata": {},
   "source": [
    "### 2. Nulls and Data Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579f1a95-f9ec-4c05-aa53-49d39aa1341b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lems          14\n",
       "selftext       0\n",
       "title          0\n",
       "title_lems    15\n",
       "author         0\n",
       "subreddit      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preg_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdfe0f74-529c-43e6-a8f4-288b912f7cef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lems          3\n",
       "selftext      0\n",
       "title         0\n",
       "title_lems    8\n",
       "author        0\n",
       "subreddit     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8859869-18d1-4e91-a383-d7b922d1c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with very few nulls in each df we can just drop the nulls, not likely to make a large impact given the large scale of data scraped \n",
    "preg_df.dropna(inplace= True)\n",
    "parent_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b6dc743-b17a-430a-a7aa-16c1b4cd4da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doublecheck that nulls dropped\n",
    "#preg_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46479c87-48aa-4880-bf03-e8cc24f2c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parent_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9548f67f-b88b-4e56-9f2a-adb32a5eb8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#previously removed from the raw data\n",
    "#only 1 'removed' that was in r/pregnant\n",
    "#preg_df[preg_df['selftext'] == '[removed]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f75e8b54-02fa-4fae-ab8b-e70841e56371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preg_df.drop(139, axis= 0, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be8c5da3-84ef-48b0-9f7a-fdaa805f6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#previously removed from the raw data\n",
    "#parent_df[parent_df['selftext'] == '[removed]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2690a68e-1595-4b2c-8644-49b768fd4a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parent_df.drop(0, axis=0, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89b0807c-ec47-403e-9c20-c340e52fd242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resetting index after the drops\n",
    "#parent_df.reset_index(inplace=True)\n",
    "#preg_df.reset_index(inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512b3d6-acbb-4ddc-a570-230f1ddacd69",
   "metadata": {},
   "source": [
    "### 3. Lemmatizing Reddit Posts from data files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe0a02e-cc43-4260-ad95-bd37841d345b",
   "metadata": {},
   "source": [
    "All of the cells below have been commented out as they were used to tokenize and lemmatize the initial raw data scrape from r/pregnant and r/parenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f3ef955-7072-4432-a3a8-e3cac185a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Lemmatizer and Tokenizer to tokenize and lem the data\n",
    "#tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e47292c7-cff6-4293-be7f-2b3c3c32663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#commented out\n",
    "#Adapted from lemma function from John Hazard reddit Project as Example\n",
    "#(https://github.com/JDHazard/web_scraping_reddit_classification_modeling/blob/master/notebooks/data_cleaning_and_eda.ipynb)\n",
    "#lemmatizer Function\n",
    "#def lemma(text):\n",
    "#    tokens = tokenizer.tokenize(text.lower())\n",
    "#    lems   = [lemmatizer.lemmatize(i) for i in tokens]\n",
    "    \n",
    "#    text = ' '.join(lems)\n",
    "#    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90e45579-f62b-43f5-bb7e-2532589d9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemma(parent_df['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0366c19b-1ae3-4d9b-9d02-5af9159ce781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing selftext for r/pregnant\n",
    "#preg_df['lems'] = [lemma(i) for i in preg_df['selftext']]\n",
    "#preg_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "176c8979-66ce-4166-b2bf-3f6067227b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing selftext for r/parenting\n",
    "#parent_df['lems'] = [lemma(i) for i in parent_df['selftext']]\n",
    "#parent_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbcfac95-841b-4dbb-bf68-c2332a5a71cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing title for r/pregnant\n",
    "#preg_df['title_lems'] = [lemma(i) for i in preg_df['title']]\n",
    "#preg_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0c082ef-8fb0-4924-ab73-86a2cf38a30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatizing title for r/parenting\n",
    "#parent_df['title_lems'] = [lemma(i) for i in parent_df['title']]\n",
    "#preg_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc0c9c-fb7c-4df6-8165-56fcff0d4de9",
   "metadata": {},
   "source": [
    "### 4. Second Data Check and Merging of r/parent and r/pregnant Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5678d-1408-4310-a98a-3b37f1a03b01",
   "metadata": {},
   "source": [
    "Some of the cells below have been commented out as they were used to tokenize and lemmatize the initial raw data scrape from r/pregnant and r/parenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bad9d318-1af8-4909-834b-7475a4725c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initially had a 'level_0' column to drop as well\n",
    "parent_df.drop(columns=['index'], inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a5137b-9f7d-47cc-86fd-9759f9672523",
   "metadata": {},
   "source": [
    "#### Rearranging Columns After Addition of Lems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "09533466-eb01-44a4-a2e8-31276f2463e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lems', 'selftext', 'title', 'title_lems', 'author', 'subreddit']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#earrange columns process from https://stackoverflow.com/questions/13148429/how-to-change-the-order-of-dataframe-columns\n",
    "cols = list(parent_df.columns)\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4f7ab58-4164-4951-ab91-787ae31e0d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author', 'lems', 'title_lems', 'subreddit', 'selftext', 'title']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [cols[-2] , cols[0], cols[3], cols[-1], cols[1] , cols[2]]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d1dcee8b-7b37-4f71-960e-a4247cdc181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rearrange columns\n",
    "preg_df = preg_df[cols]\n",
    "parent_df = parent_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a693e91-5977-4e70-be2c-0fa7044be4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sending dfs with lems to csv\n",
    "#preg_df.to_csv('./data/pregnant_lems.csv')\n",
    "#parent_df.to_csv('./data/parent_lems.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe9ff2-b288-474b-a7f2-9ef896c7d1d7",
   "metadata": {},
   "source": [
    "Next steps occurred in the Notebook2_Preproccessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
